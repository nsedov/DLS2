{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eYtJxkhKpYK2"
   },
   "source": [
    "# Embeddings\n",
    "\n",
    "Привет! В этом домашнем задании мы с помощью эмбеддингов решим задачу семантической классификации твитов.\n",
    "\n",
    "Для этого мы воспользуемся предобученными эмбеддингами word2vec."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jBOdoFS8AdpP"
   },
   "source": [
    "Для начала скачаем датасет для семантической классификации твитов:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sh6wW-K53Mle"
   },
   "source": [
    "Импортируем нужные библиотеки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "A2Y5CHRm6NFe"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import nltk\n",
    "import gensim\n",
    "import gensim.downloader as api\n",
    "\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "73Lb0wbESrgQ"
   },
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.random.manual_seed(42)\n",
    "torch.cuda.random.manual_seed(42)\n",
    "torch.cuda.random.manual_seed_all(42)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "L_Wv-4bu83Fl"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"training.1600000.processed.noemoticon.csv\", encoding=\"latin\", header=None, names=[\"emotion\", \"id\", \"date\", \"flag\", \"user\", \"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RY1pvYDS3Yuj"
   },
   "source": [
    "Посмотрим на данные:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "jST2tjgjCTWD"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emotion</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   emotion          id                          date      flag  \\\n",
       "0        0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "1        0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2        0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "3        0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4        0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "\n",
       "              user                                               text  \n",
       "0  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1    scotthamilton  is upset that he can't update his Facebook by ...  \n",
       "2         mattycus  @Kenichan I dived many times for the ball. Man...  \n",
       "3          ElleCTF    my whole body feels itchy and like its on fire   \n",
       "4           Karoli  @nationwideclass no, it's not behaving at all....  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OhbR5JJyA2VW"
   },
   "source": [
    "Выведем несколько примеров твитов, чтобы понимать, с чем мы имеем дело:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "kCBwe0wR83C2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@chrishasboobs AHHH I HOPE YOUR OK!!! \n",
      "@misstoriblack cool , i have no tweet apps  for my razr 2\n",
      "@TiannaChaos i know  just family drama. its lame.hey next time u hang out with kim n u guys like have a sleepover or whatever, ill call u\n",
      "School email won't open  and I have geography stuff on there to revise! *Stupid School* :'(\n",
      "upper airways problem \n",
      "Going to miss Pastor's sermon on Faith... \n",
      "on lunch....dj should come eat with me \n",
      "@piginthepoke oh why are you feeling like that? \n",
      "gahh noo!peyton needs to live!this is horrible \n",
      "@mrstessyman thank you glad you like it! There is a product review bit on the site  Enjoy knitting it!\n"
     ]
    }
   ],
   "source": [
    "examples = data[\"text\"].sample(10)\n",
    "print(\"\\n\".join(examples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GvcYW8aX3mKt"
   },
   "source": [
    "Как видим, тексты твитов очень \"грязные\". Нужно предобработать датасет, прежде чем строить для него модель классификации.\n",
    "\n",
    "Чтобы сравнивать различные методы обработки текста/модели/прочее, разделим датасет на dev(для обучения модели) и test(для получения качества модели)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "f8hUK-jnQg6O"
   },
   "outputs": [],
   "source": [
    "indexes = np.arange(data.shape[0])\n",
    "np.random.shuffle(indexes)\n",
    "dev_size = math.ceil(data.shape[0] * 0.8)\n",
    "\n",
    "dev_indexes = indexes[:dev_size]\n",
    "test_indexes = indexes[dev_size:]\n",
    "\n",
    "dev_data = data.iloc[dev_indexes]\n",
    "test_data = data.iloc[test_indexes]\n",
    "\n",
    "dev_data.reset_index(drop=True, inplace=True)\n",
    "test_data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ivcpeFoCnZA"
   },
   "source": [
    "## Обработка текста"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Df4nca285Dar"
   },
   "source": [
    "Токенизируем текст, избавимся от знаков пунктуации и выкинем все слова, состоящие менее чем из 4 букв:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "nsNHNDES9ZVF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@ chrishasboobs ahhh i hope your ok !!!\n"
     ]
    }
   ],
   "source": [
    "tokenizer = nltk.WordPunctTokenizer()\n",
    "line = tokenizer.tokenize(dev_data[\"text\"][0].lower())\n",
    "print(\" \".join(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "GcBS_u_hTuxp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chrishasboobs ahhh hope your\n"
     ]
    }
   ],
   "source": [
    "filtered_line = [w for w in line if all(c not in string.punctuation for c in w) and len(w) > 3]\n",
    "print(\" \".join(filtered_line))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cuFmlXkC6E7X"
   },
   "source": [
    "Загрузим предобученную модель эмбеддингов. \n",
    "\n",
    "Если хотите, можно попробовать другую. Полный список можно найти здесь: https://github.com/RaRe-Technologies/gensim-data.\n",
    "\n",
    "Данная модель выдает эмбеддинги для **слов**. Строить по эмбеддингам слов эмбеддинги предложений мы будем ниже."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "cACJpje2T5bc",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "word2vec = api.load(\"word2vec-google-news-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gensim.models.keyedvectors.KeyedVectors"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "NafmYHrkT5YD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300,)\n"
     ]
    }
   ],
   "source": [
    "emb_line = [word2vec.get_vector(w) for w in filtered_line if w in word2vec]\n",
    "print(sum(emb_line).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LTS6LCkd6_E7"
   },
   "source": [
    "Нормализуем эмбеддинги, прежде чем обучать на них сеть. \n",
    "(наверное, вы помните, что нейронные сети гораздо лучше обучаются на нормализованных данных)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "3PyLTZ6xf3Oq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300,)\n",
      "[False, False, False, False, False, False, False, False, False, False, False, False]\n"
     ]
    }
   ],
   "source": [
    "mean = np.mean(word2vec.vectors, 0)\n",
    "std = np.std(word2vec.vectors, 0)\n",
    "norm_emb_line = [(word2vec.get_vector(w) - mean) / std for w in filtered_line if w in word2vec and len(w) > 3]\n",
    "print(sum(norm_emb_line).shape)\n",
    "print([all(norm_emb_line[i] == emb_line[i]) for i in range(len(emb_line))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 300)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(norm_emb_line).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q7vm6Ppd7Ubw"
   },
   "source": [
    "Сделаем датасет, который будет по запросу возвращать подготовленные данные."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopWords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "b4eZajF7pZ1X"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, random_split\n",
    "\n",
    "\n",
    "class TwitterDataset(Dataset):\n",
    "    def __init__(self, data: pd.DataFrame, feature_column: str, target_column: str, word2vec: gensim.models.Word2Vec):\n",
    "        self.tokenizer = nltk.WordPunctTokenizer()\n",
    "        \n",
    "        self.data = data\n",
    "\n",
    "        self.feature_column = feature_column\n",
    "        self.target_column = target_column\n",
    "\n",
    "        self.word2vec = word2vec\n",
    "\n",
    "        self.label2num = lambda label: 0 if label == 0 else 1\n",
    "        self.mean = np.mean(word2vec.vectors, axis=0)\n",
    "        self.std = np.std(word2vec.vectors, axis=0)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        text = self.data[self.feature_column][item]\n",
    "        label = self.label2num(self.data[self.target_column][item])\n",
    "\n",
    "        tokens = self.get_tokens_(text)\n",
    "        embeddings = self.get_embeddings_(tokens)\n",
    "\n",
    "        return {\"feature\": embeddings, \"target\": label}\n",
    "\n",
    "    def get_tokens_(self, text):\n",
    "        text_ = re.sub(f'[{string.punctuation}]', ' ', text)\n",
    "        text_ = re.findall('\\w+', text_)\n",
    "        #text_ = \" \".join(filter(lambda x:x[0]!='@', text_))    \n",
    "        return [word.lower() for word in text_ if word.lower() not in stopWords]\n",
    "\n",
    "    def get_embeddings_(self, tokens):\n",
    "        embeddings = np.array([((word2vec[x] - self.mean) / self.std) \\\n",
    "                               for x in tokens if x in word2vec]) # Получи эмбеддинги слов и усредни их\n",
    "        if len(embeddings) != 0:\n",
    "            embeddings = sum(embeddings) / len(embeddings)\n",
    "        \n",
    "        if len(embeddings) == 0:\n",
    "            embeddings = np.zeros((1, self.word2vec.vector_size))\n",
    "        else:\n",
    "            #embeddings = np.array(embeddings)\n",
    "            if len(embeddings.shape) == 1:\n",
    "                embeddings = embeddings.reshape(1, -1)\n",
    "        return embeddings\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "IZJpttbXpZyz"
   },
   "outputs": [],
   "source": [
    "dev = TwitterDataset(dev_data, \"text\", \"emotion\", word2vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sr-aetH0_LH1"
   },
   "source": [
    "Отлично, мы готовы с помощью эмбеддингов слов превращать твиты в векторы и обучать нейронную сеть.\n",
    "\n",
    "Превращать твиты в векторы, используя эмбеддинги слов, можно несколькими способами. А именно такими:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4AhHrWa196Yc"
   },
   "source": [
    "## Average embedding (2 балла)\n",
    "---\n",
    "Это самый простой вариант, как получить вектор предложения, используя векторные представления слов в предложении. А именно: вектор предложения есть средний вектор всех слов в предлоежнии (которые остались после токенизации и удаления коротких слов, конечно). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "ScdokSW-994t"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1280\n"
     ]
    }
   ],
   "source": [
    "indexes = np.arange(len(dev))\n",
    "np.random.shuffle(indexes)\n",
    "example_indexes = indexes[::1000]\n",
    "\n",
    "examples = {\"features\": [np.sum(dev[i][\"feature\"], axis=0) for i in example_indexes], \n",
    "            \"targets\": [dev[i][\"target\"] for i in example_indexes]}\n",
    "print(len(examples[\"features\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.swapaxes(examples[\"features\"],0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.array(examples[\"targets\"]).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1yGQ_lOx_1NL"
   },
   "source": [
    "Давайте сделаем визуализацию полученных векторов твитов тренировочного (dev) датасета. Так мы увидим, насколько хорошо твиты с разными target значениями отделяются друг от друга, т.е. насколько хорошо усреднение эмбеддингов слов предложения передает информацию о предложении."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LZwFksd_8uYO"
   },
   "source": [
    "Для визуализации векторов надо получить их проекцию на плоскость. Сделаем это с помощью `PCA`. Если хотите, можете вместо PCA использовать TSNE: так у вас получится более точная проекция на плоскость (а значит, более информативная, т.е. отражающая реальное положение векторов твитов в пространстве). Но TSNE будет работать намного дольше."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "aKFZRSHdtIac"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "examples[\"transformed_features\"] = pca.fit_transform(examples['features'], examples['targets'])# Обучи PCA на эмбеддингах слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "# pca = PCA(n_components=2)\n",
    "# pca.fit(examples[\"features\"], examples[\"targets\"])\n",
    "#pca.fit(np.swapaxes(examples[\"features\"],0, 1),np.array(examples[\"targets\"]).reshape(-1, 1))#['features'], examples['targets'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "id": "szEOWdiNtIX8"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"https://bokeh.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"1003\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  const force = true;\n",
       "\n",
       "  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "  const JS_MIME_TYPE = 'application/javascript';\n",
       "  const HTML_MIME_TYPE = 'text/html';\n",
       "  const EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  const CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    const script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    const cell = handle.cell;\n",
       "\n",
       "    const id = cell.output_area._bokeh_element_id;\n",
       "    const server_id = cell.output_area._bokeh_server_id;\n",
       "    // Clean up Bokeh references\n",
       "    if (id != null && id in Bokeh.index) {\n",
       "      Bokeh.index[id].model.document.clear();\n",
       "      delete Bokeh.index[id];\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      const cmd_clean = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd_clean, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            const id = msg.content.text.trim();\n",
       "            if (id in Bokeh.index) {\n",
       "              Bokeh.index[id].model.document.clear();\n",
       "              delete Bokeh.index[id];\n",
       "            }\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      const cmd_destroy = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd_destroy);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    const output_area = handle.output_area;\n",
       "    const output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!Object.prototype.hasOwnProperty.call(output.data, EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    const toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      const bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      const script_attrs = bk_div.children[0].attributes;\n",
       "      for (let i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      const toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      const props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[toinsert.length - 1]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    const events = require('base/js/events');\n",
       "    const OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  \n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  const NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    const el = document.getElementById(\"1003\");\n",
       "    if (el != null) {\n",
       "      el.textContent = \"BokehJS is loading...\";\n",
       "    }\n",
       "    if (root.Bokeh !== undefined) {\n",
       "      if (el != null) {\n",
       "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) {\n",
       "        if (callback != null)\n",
       "          callback();\n",
       "      });\n",
       "    } finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.debug(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(css_urls, js_urls, callback) {\n",
       "    if (css_urls == null) css_urls = [];\n",
       "    if (js_urls == null) js_urls = [];\n",
       "\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = css_urls.length + js_urls.length;\n",
       "\n",
       "    function on_load() {\n",
       "      root._bokeh_is_loading--;\n",
       "      if (root._bokeh_is_loading === 0) {\n",
       "        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n",
       "        run_callbacks()\n",
       "      }\n",
       "    }\n",
       "\n",
       "    function on_error(url) {\n",
       "      console.error(\"failed to load \" + url);\n",
       "    }\n",
       "\n",
       "    for (let i = 0; i < css_urls.length; i++) {\n",
       "      const url = css_urls[i];\n",
       "      const element = document.createElement(\"link\");\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error.bind(null, url);\n",
       "      element.rel = \"stylesheet\";\n",
       "      element.type = \"text/css\";\n",
       "      element.href = url;\n",
       "      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n",
       "      document.body.appendChild(element);\n",
       "    }\n",
       "\n",
       "    for (let i = 0; i < js_urls.length; i++) {\n",
       "      const url = js_urls[i];\n",
       "      const element = document.createElement('script');\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error.bind(null, url);\n",
       "      element.async = false;\n",
       "      element.src = url;\n",
       "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.head.appendChild(element);\n",
       "    }\n",
       "  };\n",
       "\n",
       "  function inject_raw_css(css) {\n",
       "    const element = document.createElement(\"style\");\n",
       "    element.appendChild(document.createTextNode(css));\n",
       "    document.body.appendChild(element);\n",
       "  }\n",
       "\n",
       "  \n",
       "  const js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.4.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-2.4.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.4.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.4.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-2.4.1.min.js\"];\n",
       "  const css_urls = [];\n",
       "  \n",
       "\n",
       "  const inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    function(Bokeh) {\n",
       "    \n",
       "    \n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if (root.Bokeh !== undefined || force === true) {\n",
       "      \n",
       "    for (let i = 0; i < inline_js.length; i++) {\n",
       "      inline_js[i].call(root, root.Bokeh);\n",
       "    }\n",
       "    if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      const cell = $(document.getElementById(\"1003\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(css_urls, js_urls, function() {\n",
       "      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  const force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  \n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  const NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    const el = document.getElementById(\"1003\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error(url) {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (let i = 0; i < css_urls.length; i++) {\n      const url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    for (let i = 0; i < js_urls.length; i++) {\n      const url = js_urls[i];\n      const element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  \n  const js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.4.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-2.4.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.4.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.4.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-2.4.1.min.js\"];\n  const css_urls = [];\n  \n\n  const inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    function(Bokeh) {\n    \n    \n    }\n  ];\n\n  function run_inline_js() {\n    \n    if (root.Bokeh !== undefined || force === true) {\n      \n    for (let i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\n    if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      const cell = $(document.getElementById(\"1003\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import bokeh.models as bm, bokeh.plotting as pl\n",
    "from bokeh.io import output_notebook\n",
    "output_notebook()\n",
    "\n",
    "def draw_vectors(x, y, radius=10, alpha=0.25, color='blue',\n",
    "                 width=600, height=400, show=True, **kwargs):\n",
    "    \"\"\" draws an interactive plot for data points with auxilirary info on hover \"\"\"\n",
    "    data_source = bm.ColumnDataSource({ 'x' : x, 'y' : y, 'color': color, **kwargs })\n",
    "\n",
    "    fig = pl.figure(active_scroll='wheel_zoom', width=width, height=height)\n",
    "    fig.scatter('x', 'y', size=radius, color='color', alpha=alpha, source=data_source)\n",
    "\n",
    "    fig.add_tools(bm.HoverTool(tooltips=[(key, \"@\" + key) for key in kwargs.keys()]))\n",
    "    if show: pl.show(fig)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "id": "7OONK8ldtIWe"
   },
   "outputs": [],
   "source": [
    "# draw_vectors(\n",
    "#     examples[\"transformed_features\"][:, 0], \n",
    "#     examples[\"transformed_features\"][:, 1], \n",
    "#     color=[[\"red\", \"blue\"][t] for t in examples[\"targets\"]]\n",
    "#     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0fNF6LRQ9MPI"
   },
   "source": [
    "Скорее всего, на визуализации нет четкого разделения твитов между классами. Это значит, что по полученным нами векторам твитов не так-то просто определить, к какому классу твит пренадлежит. Значит, обычный линейный классификатор не очень хорошо справится с задачей. Надо будет делать глубокую (хотя бы два слоя) нейронную сеть.\n",
    "\n",
    "Подготовим загрузчики данных.\n",
    "Усреднее векторов будем делать в \"батчевалке\"(`collate_fn`). Она используется для того, чтобы собирать из данных `torch.Tensor` батчи, которые можно отправлять в модель.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "y1XapsADtITv"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "batch_size = 1024\n",
    "num_workers = 0\n",
    "\n",
    "def average_emb(batch):\n",
    "    features = np.array([np.mean(b[\"feature\"], axis=0) for b in batch])\n",
    "    targets = np.array([b[\"target\"] for b in batch]).astype(float)\n",
    "\n",
    "    #return {\"features\": torch.FloatTensor(features), \"targets\": torch.LongTensor(targets)}\n",
    "    return (torch.FloatTensor(features), torch.LongTensor(targets))\n",
    "\n",
    "\n",
    "train_size = math.ceil(len(dev) * 0.8)\n",
    "\n",
    "train, valid = random_split(dev, [train_size, len(dev) - train_size])\n",
    "\n",
    "train_loader = DataLoader(train, batch_size=batch_size, num_workers=num_workers, shuffle=True, drop_last=True, collate_fn=average_emb)\n",
    "valid_loader = DataLoader(valid, batch_size=batch_size, num_workers=num_workers, shuffle=False, drop_last=False, collate_fn=average_emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p-zs0WEK-Vkt"
   },
   "source": [
    "Определим функции для тренировки и теста модели:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "U--T2Gjw1r27"
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "def training(model, optimizer, criterion, scheduler, train_loader, epoch, device=\"cuda\"):\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {e + 1}. Train Loss: {0}\")\n",
    "    model.train()\n",
    "    for batch in pbar:\n",
    "        features = batch[0].to(device)\n",
    "        targets = batch[1].to(torch.float32).to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(features) # Получи предсказания модели\n",
    "        #print(type(targets[0][0]))\n",
    "        loss = criterion(predictions, targets) # Посчитай лосс\n",
    "        loss.backward() # Обнови параметры модели\n",
    "        optimizer.step()\n",
    "        pbar.set_description(f\"Epoch {e + 1}. Train Loss: {loss.cpu():.4}\")\n",
    "    scheduler.step()\n",
    "    \n",
    "\n",
    "def testing(model, criterion, test_loader, device=\"cuda\"):\n",
    "    pbar = tqdm(test_loader, desc=f\"Test Loss: {0}, Test Acc: {0}\")\n",
    "    mean_loss = 0\n",
    "    mean_acc = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in pbar:\n",
    "            features = batch[0].to(device)\n",
    "            targets = batch[1].to(torch.float32).to(device)\n",
    "\n",
    "            predictions = model(features) # Получи предсказания модели\n",
    "            #print(type(targets[0]))\n",
    "            loss = criterion(predictions, targets) # Посчитай лосс\n",
    "            targets = torch.round(targets.cpu())\n",
    "            predictions = torch.round(predictions.cpu())\n",
    "            acc = accuracy_score(targets, predictions) # Посчитай точность модели\n",
    "\n",
    "            mean_loss += loss.item()\n",
    "            mean_acc += acc.item()\n",
    "\n",
    "            pbar.set_description(f\"Test Loss: {loss:.4}, Test Acc: {acc:.4}\")\n",
    "\n",
    "    pbar.set_description(f\"Test Loss: {mean_loss / len(test_loader):.4}, Test Acc: {mean_acc / len(test_loader):.4}\")\n",
    "\n",
    "    return {\"Test Loss\": mean_loss / len(test_loader), \"Test Acc\": mean_acc / len(test_loader)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oVg_XBBb-YBH"
   },
   "source": [
    "Создадим модель, оптимизатор и целевую функцию. Вы можете сами выбрать количество слоев в нейронной сети, ваш любимый оптимизатор и целевую функцию.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_size = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN(nn.Module):\n",
    "  \n",
    "    def __init__(self, n_classes, dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(dim,  layer_size),\n",
    "            nn.BatchNorm1d(layer_size),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(layer_size,  layer_size),\n",
    "            nn.BatchNorm1d(layer_size),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.fc3 = nn.Sequential(\n",
    "            nn.Linear(layer_size,  layer_size),\n",
    "            nn.BatchNorm1d(layer_size),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.out = nn.Linear(layer_size,  n_classes)\n",
    "        self.sigm = nn.Sigmoid()\n",
    "    def forward(self, x):\n",
    "        # print(x)\n",
    "        # x = x.permute(0, 2, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        x_out = self.out(x)\n",
    "        return torch.reshape(self.sigm(x_out), (-1,))\n",
    "        #return torch.reshape(x_out, (-1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "id": "EBoZ4F3Fx1Hm"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "\n",
    "\n",
    "# Не забудь поиграться с параметрами ;)\n",
    "vector_size = dev.word2vec.vector_size\n",
    "num_classes = 1\n",
    "lr = 1e-2\n",
    "num_epochs = 3\n",
    "\n",
    "model = NN(num_classes, 300) # Твоя модель\n",
    "model = model.cuda()\n",
    "criterion =  nn.BCELoss() # Твой лосс\n",
    "optimizer = Adam(model.parameters(), lr = lr) # Твой оптимайзер"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-AitU8AR-zBj"
   },
   "source": [
    "Наконец, обучим модель и протестируем её.\n",
    "\n",
    "После каждой эпохи будем проверять качество модели на валидационной части датасета. Если метрика стала лучше, будем сохранять модель. **Подумайте, какая метрика (точность или лосс) будет лучше работать в этой задаче?** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "id": "gKhk71Pmx1F1"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85ed6463d1b64ce4b852c0dae3195e71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1. Train Loss: 0:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba88fe0d5b6e499288c700942a415313",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Test Loss: 0, Test Acc: 0:   0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Test Loss': 0.4918893357515335, 'Test Acc': 0.75840625}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5aebec5a771842659470664cacca1dba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2. Train Loss: 0:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "163e97f4221c4b29ac5037b30e4f1baa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Test Loss: 0, Test Acc: 0:   0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Test Loss': 0.47863507437705993, 'Test Acc': 0.76667578125}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "577afcd988344399b0cf1d9c6b568592",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3. Train Loss: 0:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c13598c649246c68afaff8821d9b037",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Test Loss: 0, Test Acc: 0:   0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Test Loss': 0.4771719051599503, 'Test Acc': 0.7673203125}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba39268221ff49d6909e7ab09f61b9c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4. Train Loss: 0:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4a74be00c7241d696b96ebb5a7e10a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Test Loss: 0, Test Acc: 0:   0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Test Loss': 0.47732950627803805, 'Test Acc': 0.767609375}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9ea2150f434490dbcfaea144c08e095",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5. Train Loss: 0:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c5c36e4168041c48650da990041fa90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Test Loss: 0, Test Acc: 0:   0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Test Loss': 0.47723623836040496, 'Test Acc': 0.76756640625}\n"
     ]
    }
   ],
   "source": [
    "best_metric = np.inf\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size = 3, gamma=0.1)\n",
    "for e in range(num_epochs):\n",
    "    training(model, optimizer, criterion, scheduler, train_loader, e, device)\n",
    "    log = testing(model, criterion, valid_loader, device)\n",
    "    print(log)\n",
    "    if log[\"Test Loss\"] < best_metric:\n",
    "        torch.save(model.state_dict(), \"model.pt\")\n",
    "        best_metric = log[\"Test Loss\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "id": "di4dGwD4x1Dt",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09d62f8fef5a4873bc13cbffc625558a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Test Loss: 0, Test Acc: 0:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Test Loss': 0.47905410421542083, 'Test Acc': 0.7660025209664537}\n"
     ]
    }
   ],
   "source": [
    "test_loader = DataLoader(\n",
    "    TwitterDataset(test_data, \"text\", \"emotion\", word2vec), \n",
    "    batch_size=batch_size, \n",
    "    num_workers=num_workers, \n",
    "    shuffle=False,\n",
    "    drop_last=False, \n",
    "    collate_fn=average_emb)\n",
    "\n",
    "model.load_state_dict(torch.load(\"model.pt\", map_location=device))\n",
    "\n",
    "print(testing(model, criterion, test_loader, device=device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZRvzpldHSAu0"
   },
   "source": [
    "## Embeddings for unknown words (8 баллов)\n",
    "\n",
    "Пока что использовалась не вся информация из текста. Часть информации фильтровалось – если слова не было в словаре эмбеддингов, то мы просто превращали слово в нулевой вектор. Хочется использовать информацию по-максимуму. Поэтому рассмотрим другие способы обработки слов, которых нет в словаре. А именно:\n",
    "\n",
    "- Для каждого незнакомого слова будем запоминать его контекст(слова слева и справа от этого слова). Эмбеддингом нашего незнакомого слова будет сумма эмбеддингов всех слов из его контекста. (4 балла)\n",
    "- Для каждого слова текста получим его эмбеддинг из Tfidf с помощью ```TfidfVectorizer``` из [sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer). Итоговым эмбеддингом для каждого слова будет сумма двух эмбеддингов: предобученного и Tfidf-ного. Для слов, которых нет в словаре предобученных эмбеддингов, результирующий эмбеддинг будет просто полученный из Tfidf. (4 балла)\n",
    "\n",
    "Реализуйте оба варианта **ниже**. Напишите, какой способ сработал лучше и ваши мысли, почему так получилось."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "id": "RxhEpKalU1UQ"
   },
   "outputs": [],
   "source": [
    "class TwitterDataset(Dataset):\n",
    "    def __init__(self, data: pd.DataFrame, feature_column: str, target_column: str, word2vec: gensim.models.Word2Vec):\n",
    "        self.tokenizer = nltk.WordPunctTokenizer()\n",
    "        \n",
    "        self.data = data\n",
    "\n",
    "        self.feature_column = feature_column\n",
    "        self.target_column = target_column\n",
    "\n",
    "        self.word2vec = word2vec\n",
    "\n",
    "        self.label2num = lambda label: 0 if label == 0 else 1\n",
    "        self.mean = np.mean(word2vec.vectors, axis=0)\n",
    "        self.std = np.std(word2vec.vectors, axis=0)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        text = self.data[self.feature_column][item]\n",
    "        label = self.label2num(self.data[self.target_column][item])\n",
    "\n",
    "        tokens = self.get_tokens_(text)\n",
    "        embeddings = self.get_embeddings_(tokens)\n",
    "\n",
    "        return {\"feature\": embeddings, \"target\": label}\n",
    "\n",
    "    def get_tokens_(self, text):\n",
    "        text_ = re.sub(f'[{string.punctuation}]', ' ', text)\n",
    "        text_ = re.findall('\\w+', text_) \n",
    "        return [word.lower() for word in text_ if word.lower() not in stopWords]\n",
    "\n",
    "    def get_embeddings_(self, tokens):\n",
    "        embeddings = []\n",
    "        ind = [] \n",
    "        for i, word in enumerate(tokens):\n",
    "            if word in word2vec:\n",
    "                embeddings.append(word2vec[word])\n",
    "                ind.append(i)\n",
    "        for i, word in enumerate(tokens):\n",
    "            if word not in word2vec:\n",
    "                vec = np.zeros(300)\n",
    "                lower_bound = max([-1] + [x for x in ind if x < i])\n",
    "                upper_bound = min([len(tokens)] + [x for x in ind if x > i])\n",
    "                nb = False \n",
    "                if (lower_bound > -1):\n",
    "                    vec += word2vec[tokens[lower_bound]]\n",
    "                    nb = True \n",
    "                if (upper_bound < len(tokens)):\n",
    "                    vec += word2vec[tokens[upper_bound]]\n",
    "                    nb = True\n",
    "                if nb:\n",
    "                    embeddings.append(vec)\n",
    "        #embeddings = np.array([word2vec[x] for x in tokens if x in word2vec]) # Получи эмбеддинги слов и усредни их\n",
    "        if len(embeddings) != 0:\n",
    "            embeddings = sum(embeddings) / len(embeddings)\n",
    "        \n",
    "        if len(embeddings) == 0:\n",
    "            embeddings = np.zeros((1, self.word2vec.vector_size))\n",
    "        else:\n",
    "            #embeddings = np.array(embeddings)\n",
    "            if len(embeddings.shape) == 1:\n",
    "                embeddings = embeddings.reshape(1, -1)\n",
    "        return embeddings\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = TwitterDataset(dev_data, \"text\", \"emotion\", word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = 0\n",
    "\n",
    "def average_emb(batch):\n",
    "    features = np.array([np.mean(b[\"feature\"], axis=0) for b in batch])\n",
    "    targets = np.array([b[\"target\"] for b in batch]).astype(float)\n",
    "\n",
    "    #return {\"features\": torch.FloatTensor(features), \"targets\": torch.LongTensor(targets)}\n",
    "    return (torch.FloatTensor(features), torch.LongTensor(targets))\n",
    "\n",
    "\n",
    "train_size = math.ceil(len(dev) * 0.8)\n",
    "\n",
    "train, valid = random_split(dev, [train_size, len(dev) - train_size])\n",
    "\n",
    "train_loader = DataLoader(train, batch_size=batch_size, num_workers=num_workers, shuffle=True, drop_last=True, collate_fn=average_emb)\n",
    "valid_loader = DataLoader(valid, batch_size=batch_size, num_workers=num_workers, shuffle=False, drop_last=False, collate_fn=average_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Не забудь поиграться с параметрами ;)\n",
    "vector_size = dev.word2vec.vector_size\n",
    "num_classes = 1\n",
    "lr = 1e-2\n",
    "num_epochs = 3\n",
    "\n",
    "model = NN(num_classes, 300) # Твоя модель\n",
    "model = model.cuda()\n",
    "criterion =  nn.BCELoss() # Твой лосс\n",
    "optimizer = Adam(model.parameters(), lr = lr) # Твой оптимайзер"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9df818200be43e39fdde19d30d14aad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1. Train Loss: 0:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e36cf8aa4aaf464ea075dfdd0284fa1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Test Loss: 0, Test Acc: 0:   0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Test Loss': 0.5054535182714462, 'Test Acc': 0.74821484375}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30c2d508ad30490ca1287a0e92b38e79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2. Train Loss: 0:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87e66887396842a8a8d49192f99068be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Test Loss: 0, Test Acc: 0:   0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Test Loss': 0.49391808116436003, 'Test Acc': 0.75623828125}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dd19a13ae1d4b3d9a9acb92586e8158",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3. Train Loss: 0:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a73687c52a54ca59c8f1baaa3011464",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Test Loss: 0, Test Acc: 0:   0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Test Loss': 0.4849168733358383, 'Test Acc': 0.7625}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "896ab76096ab4f60bd51b2ee1583085a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4. Train Loss: 0:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fe40935b40844debd61cc5dd548989a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Test Loss: 0, Test Acc: 0:   0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Test Loss': 0.48566075456142427, 'Test Acc': 0.76310546875}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6490e9b0e38441cca52b1d34d83ade13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5. Train Loss: 0:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "699c2c00e4084abe9d542580244d257d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Test Loss: 0, Test Acc: 0:   0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Test Loss': 0.484594456076622, 'Test Acc': 0.76433203125}\n"
     ]
    }
   ],
   "source": [
    "best_metric = np.inf\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size = 3, gamma=0.5)\n",
    "for e in range(num_epochs):\n",
    "    training(model, optimizer, criterion, scheduler, train_loader, e, device)\n",
    "    log = testing(model, criterion, valid_loader, device)\n",
    "    print(log)\n",
    "    if log[\"Test Loss\"] < best_metric:\n",
    "        torch.save(model.state_dict(), \"model.pt\")\n",
    "        best_metric = log[\"Test Loss\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d839fd1c2ef4164b20dc1aec32391f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Test Loss: 0, Test Acc: 0:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Test Loss': 0.48383807488523733, 'Test Acc': 0.7644612370207667}\n"
     ]
    }
   ],
   "source": [
    "test_loader = DataLoader(\n",
    "    TwitterDataset(test_data, \"text\", \"emotion\", word2vec), \n",
    "    batch_size=batch_size, \n",
    "    num_workers=num_workers, \n",
    "    shuffle=False,\n",
    "    drop_last=False, \n",
    "    collate_fn=average_emb)\n",
    "\n",
    "model.load_state_dict(torch.load(\"model.pt\", map_location=device))\n",
    "\n",
    "print(testing(model, criterion, test_loader, device=device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "684358"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(np.array(data[\"text\"]))\n",
    "len(vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(n_components=300, n_iter=25)\n",
    "tr_emb = svd.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, random_split\n",
    "\n",
    "\n",
    "class TwitterDataset(Dataset):\n",
    "    def __init__(self, data: pd.DataFrame, feature_column: str, target_column: str,\\\n",
    "                 word2vec: gensim.models.Word2Vec, tr_emb: np.ndarray, voc):\n",
    "        self.tokenizer = nltk.WordPunctTokenizer()\n",
    "        \n",
    "        self.data = data\n",
    "\n",
    "        self.feature_column = feature_column\n",
    "        self.target_column = target_column\n",
    "\n",
    "        self.word2vec = word2vec\n",
    "\n",
    "        self.label2num = lambda label: 0 if label == 0 else 1\n",
    "        self.mean = np.mean(word2vec.vectors, axis=0)\n",
    "        self.std = np.std(word2vec.vectors, axis=0)\n",
    "        \n",
    "        self.tr_mean = np.mean(tr_emb, axis=0)\n",
    "        self.tr_std = np.std(tr_emb, axis=0)\n",
    "        \n",
    "        self.voc = voc\n",
    "        self.tr_emb = tr_emb\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        text = self.data[self.feature_column][item]\n",
    "        label = self.label2num(self.data[self.target_column][item])\n",
    "\n",
    "        tokens = self.get_tokens_(text)\n",
    "        embeddings = self.get_embeddings_(tokens)\n",
    "\n",
    "        return {\"feature\": embeddings, \"target\": label}\n",
    "\n",
    "    def get_tokens_(self, text):\n",
    "        text_ = re.sub(f'[{string.punctuation}]', ' ', text)\n",
    "        text_ = re.findall('\\w+', text_)\n",
    "\n",
    "        return [word.lower() for word in text_ if word.lower() not in stopWords]\n",
    "\n",
    "    def get_embeddings_(self, tokens):\n",
    "        # Получи эмбеддинги слов и усредни их\n",
    "#         embeddings = np.array([((word2vec[x] - self.mean) / self.std) \\\n",
    "#                                for x in tokens if x in word2vec]) \n",
    "        embeddings = []\n",
    "        #print(tokens)\n",
    "        for w in tokens:\n",
    "            if w not in self.voc :\n",
    "                continue\n",
    "            nw = (self.tr_emb[self.voc[w]] - self.tr_mean) / self.tr_std\n",
    "            if w in self.word2vec:\n",
    "                nw += (self.word2vec[w] - self.mean) / self.std \n",
    "            embeddings.append(nw)\n",
    "            \n",
    "        if len(embeddings) != 0:\n",
    "            embeddings = sum(embeddings) / len(embeddings)\n",
    "        \n",
    "        if len(embeddings) == 0:\n",
    "            embeddings = np.zeros((1, self.word2vec.vector_size))\n",
    "        else:\n",
    "            #embeddings = np.array(embeddings)\n",
    "            if len(embeddings.shape) == 1:\n",
    "                embeddings = embeddings.reshape(1, -1)\n",
    "        return embeddings\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = TwitterDataset(dev_data, \"text\", \"emotion\", word2vec, tr_emb = tr_emb, voc = vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "batch_size = 1024\n",
    "num_workers = 0\n",
    "\n",
    "def average_emb(batch):\n",
    "    features = np.array([np.mean(b[\"feature\"], axis=0) for b in batch])\n",
    "    targets = np.array([b[\"target\"] for b in batch]).astype(float)\n",
    "\n",
    "    #return {\"features\": torch.FloatTensor(features), \"targets\": torch.LongTensor(targets)}\n",
    "    return (torch.FloatTensor(features), torch.LongTensor(targets))\n",
    "\n",
    "\n",
    "train_size = math.ceil(len(dev) * 0.8)\n",
    "\n",
    "train, valid = random_split(dev, [train_size, len(dev) - train_size])\n",
    "\n",
    "train_loader = DataLoader(train, batch_size=batch_size, num_workers=num_workers, shuffle=True, drop_last=True, collate_fn=average_emb)\n",
    "valid_loader = DataLoader(valid, batch_size=batch_size, num_workers=num_workers, shuffle=False, drop_last=False, collate_fn=average_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "vector_size = dev.word2vec.vector_size\n",
    "num_classes = 1\n",
    "lr = 1e-2\n",
    "num_epochs = 3\n",
    "\n",
    "model = NN(num_classes, 300) # Твоя модель\n",
    "model = model.cuda()\n",
    "criterion =  nn.BCELoss() # Твой лосс\n",
    "optimizer = Adam(model.parameters(), lr = lr) # Твой оптимайзер"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78166541229c4c508498cc4c2b1ae67f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1. Train Loss: 0:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f68f867b31d40f5ba14a6797941da6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Test Loss: 0, Test Acc: 0:   0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Test Loss': 0.5246816556453705, 'Test Acc': 0.73408984375}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40f26fdcf6534384b1cc9bbc6756d4c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2. Train Loss: 0:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c68c15151374419b82c6338dfda4242b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Test Loss: 0, Test Acc: 0:   0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Test Loss': 0.5169335196018219, 'Test Acc': 0.7403125}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d62583c0f4e4ab7a62eb7af13b7e731",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3. Train Loss: 0:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d9aadfc83f844a9be09e1f033812579",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Test Loss: 0, Test Acc: 0:   0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Test Loss': 0.5058239589929581, 'Test Acc': 0.74678515625}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb33f3a886d44607b37de0ee4297da26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4. Train Loss: 0:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17676c0f96eb4cc492a791ad4f842ad0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Test Loss: 0, Test Acc: 0:   0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Test Loss': 0.5038978643417359, 'Test Acc': 0.74878515625}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95f8c66772474d28bd515e91ed461161",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5. Train Loss: 0:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd616bbfd7a142e9b30e7c2ffd6e0ddc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Test Loss: 0, Test Acc: 0:   0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Test Loss': 0.5015738022327423, 'Test Acc': 0.75231640625}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34a09e20828f4372a668d392b96b9bc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6. Train Loss: 0:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7baf8780fccc4f6b811414ff161cb840",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Test Loss: 0, Test Acc: 0:   0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Test Loss': 0.5021106672286987, 'Test Acc': 0.75246875}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba2102089f424dfb86a088f2ca103974",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7. Train Loss: 0:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5d1fe14744d420b8e327c62d0e64a1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Test Loss: 0, Test Acc: 0:   0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Test Loss': 0.5039725112915039, 'Test Acc': 0.75265625}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee4821ba3ca94d30a46eaea5a3365eb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8. Train Loss: 0:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d73e626b8dbf481582175f7f9cce9b93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Test Loss: 0, Test Acc: 0:   0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Test Loss': 0.5042626264095307, 'Test Acc': 0.75218359375}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5fde425575345e586d77b91830f58eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9. Train Loss: 0:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7411300684d146ed98320e3054596a01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Test Loss: 0, Test Acc: 0:   0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Test Loss': 0.5078626008033752, 'Test Acc': 0.75121484375}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e49f8f61b0341aabbebfe37b97cb95b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10. Train Loss: 0:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "464c6b55ad6f4afeaf1914123c80485b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Test Loss: 0, Test Acc: 0:   0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Test Loss': 0.5083010985851287, 'Test Acc': 0.75069921875}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75850fe2dc3442dbbd81419260cc73ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 11. Train Loss: 0:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99da5445aff94cce85870081e38e8013",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Test Loss: 0, Test Acc: 0:   0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Test Loss': 0.5088596085309982, 'Test Acc': 0.75079296875}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08b8fe5c72c849a993aeef9ac71bcefc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 12. Train Loss: 0:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7df77c76f3204fca927accb6633e8eca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Test Loss: 0, Test Acc: 0:   0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Test Loss': 0.5092762076854705, 'Test Acc': 0.75063671875}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3196729068f7494ab7c5655fe368972c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 13. Train Loss: 0:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8a7bc6a3acf431eb94fff0798d01f87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Test Loss: 0, Test Acc: 0:   0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Test Loss': 0.5098575885295868, 'Test Acc': 0.75063671875}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25be8fb692a94eef85d49aff02f2be50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 14. Train Loss: 0:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65d9fc73ef764d6db5ee2c6c13224188",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Test Loss: 0, Test Acc: 0:   0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Test Loss': 0.5096657925844192, 'Test Acc': 0.75066015625}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b8418215a64445d95240ddb90a9dc8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 15. Train Loss: 0:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52043c425db041ac894cc82ec89504a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Test Loss: 0, Test Acc: 0:   0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Test Loss': 0.5095833743810654, 'Test Acc': 0.7504140625}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80c1265e184d4a47985da09503b0b38b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 16. Train Loss: 0:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d14398735af74216bfc5661e2b0add63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Test Loss: 0, Test Acc: 0:   0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_metric = np.inf\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size = 3, gamma=0.3)\n",
    "for e in range(num_epochs):\n",
    "    training(model, optimizer, criterion, scheduler, train_loader, e, device)\n",
    "    log = testing(model, criterion, valid_loader, device)\n",
    "    print(log)\n",
    "    if log[\"Test Loss\"] < best_metric:\n",
    "        torch.save(model.state_dict(), \"model.pt\")\n",
    "        best_metric = log[\"Test Loss\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "[homework]embeddings.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
